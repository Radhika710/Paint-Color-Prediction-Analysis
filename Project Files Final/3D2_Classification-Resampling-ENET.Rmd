---
title: "INFSCI 2595 Final Project"
subtitle: "Part 3D1, Classification - Resampling - ENET"
author: "Radhika Purohit"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages}
library(tidyverse)
theme_set(theme_linedraw())
library(tidymodels)
tidymodels_prefer()
```

```{r, make_iiiD_data}
df <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)
dfiiiD <- df %>% 
  select(-response) %>% 
  mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))

dfiiiD %>% glimpse()
```

By converting `outcome` to a factor, the unique values of the variables are "always known":  

```{r, show_outcome_levels}
dfiiiD %>% pull(outcome) %>% levels()
```

However, the value counts are the same as the original encoding.  

```{r, confirm_outcome_Counts}
dfiiiD %>% count(outcome)
```


## Elastic Net

Use Elastic Net to provide regularization to the two complex logistic models that resulted poor performances during the resampling.  

### Models

**Define the linear models to tune with Elastic Net.**  

```{r preprocess}
bp_prep_cl <- recipe(outcome ~ ., data=dfiiiD) %>%
  step_normalize(all_numeric_predictors())
  #prep(training=df_clas, retain=T) %>% bake(new_data=NULL)
```


```{r mod_3D2}
bp_3D1 <- bp_prep_cl %>%
  step_interact(~ all_numeric_predictors():all_numeric_predictors(), sep = ":") %>%
  step_dummy(all_nominal_predictors())

```


```{r}
bp_3D2 <- bp_prep_cl %>%
  step_dummy(Lightness, one_hot = T) %>%
  step_interact(~ starts_with("Lightness") : all_numeric_predictors(), sep = ":") %>%
  step_dummy(Saturation)
```

----

## Tuning

### Explore

**First use Lasso to explore the range for parameter tuning.**  

Define the Lasso only model specification.  

```{r}
lasso_explore_spec_cl <-
  logistic_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet", intercept = T, standardize = F, family = "binomial") %>%
  set_mode("classification")
```

Fit both models with Lasso.

```{r}
lasso_fit_3D1 <- workflow() %>%
  add_model(lasso_explore_spec_cl) %>%
  add_recipe(bp_3D1) %>%
  fit(dfiiiD)

lasso_fit_3D2 <- workflow() %>%
  add_model(lasso_explore_spec_cl) %>%
  add_recipe(bp_3D2) %>%
  fit(dfiiiD)
```

Generate the coefficient paths.

```{r}
lasso_fit_3D1 %>% extract_fit_parsnip() %>%
  pluck("fit") %>%
  plot(xvar = "lambda")
lasso_fit_3D2 %>% extract_fit_parsnip() %>%
  pluck("fit") %>%
  plot(xvar = "lambda")
```
### Setup

**Create the custom search grid for the penalty strength and mixing fraction parameters.**  

- Use the bound of penalty strength for the range of parameters I am interested to tune.
- Use 0.1 as lower bound for mixing fraction to avoid fully Ridge.  

```{r tuning grid}
# set tuning parameter ranges
my_lambda <- penalty(range = c(-8,-1), trans = log_trans())
my_alpha <- dials::mixture(range = c(0.1, 1.0))
# construct the tuning grid
enet_grid_cl <- grid_regular(
  my_lambda, my_alpha,
  levels = c(penalty = 60, mixture = 5))
```

**Define the resampling scheme**  

- Use the 5-fold cross-validation with 5 repeats.
- Model specification: use `glmnet` engine.

```{r resampling}
set.seed(1324)
cv_folds <- vfold_cv(dfiiiD, v = 5, repeats = 5)
my_metrics_cl <- metric_set(accuracy, roc_auc, mn_log_loss)
```

**Create workflow set.**  

```{r workflow}
enet_spec_cl <-
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet", intercept = T, standardize = F, family = "binomial") %>%
  set_mode("classification")

enet_wset_cl <-
  workflow_set(
    preproc = list(
      all_cont_pairwise = bp_3D1,
      inter_categorical_cont = bp_3D2
      ),
    models = list(enet = enet_spec_cl)
  )
```

### Execute

```{r execute, eval=TRUE}
# create cluster to allow parallel computing
if (parallel::detectCores(logical=FALSE) > 3) {
  library(doParallel)
  num_cores <- parallel::detectCores(logical=FALSE)
  cl <- makePSOCKcluster(num_cores - 2)
  registerDoParallel(cl)
}
# start tuning
tune_3D_enet <- enet_wset_cl %>%
  workflow_map(
    fn = "tune_grid",
    grid = enet_grid_cl,
    resamples = cv_folds,
    metrics = my_metrics_cl,
    control = control_grid(
      # save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
    ),
    verbose = T
  )
# close the cluster
stopCluster(cl)
registerDoSEQ()
```

----

## Conclusions

### Performance

**Visualize the model performance metrics across the two models tuned.** 

```{r}
names(tune_3D_enet$result) <- tune_3D_enet$wflow_id

tune_3D_enet$result %>%
  map_dfr(collect_metrics, .id="wflow_id") %>%
  ggplot(aes(x = log(penalty))) +
  geom_ribbon(
    aes(ymin = mean - std_err, ymax = mean + std_err,
        group = interaction(mixture, .metric),
        fill = as.factor(mixture)),
    alpha = 0.2) +
  geom_line(
    aes(y = mean,
        group = interaction(mixture, .metric),
        color = as.factor(mixture)),
    size = 1) +
  facet_grid(.metric ~ wflow_id, scales = "free_y") +
  xlab("log-penalty") + ylab("performance") + labs(color="mixture", fill="mixture") +
  scale_color_viridis_d() + scale_fill_viridis_d()
```

- The performances are similar for the two models. 

### Best Model

**Select the best tuning parameters for each model based on mean log loss.**  

```{r}
enet_best_roc_params <-
  tune_3D_enet$result %>%
  map_dfr(select_by_one_std_err, desc(mixture), metric = "mn_log_loss", .id = "wflow_id")
enet_best_roc_params %>% select(-starts_with("."))
```

- For the model with pairwise interaction of all continuous inputs, a higher mixing rate is preferred, indicating more Lasso to shut off features.  

**Visualize all three performance metrics for the best models.**

```{r }
tune_3D_enet %>% collect_metrics() %>% 
  inner_join(enet_best_roc_params %>% select(wflow_id, .config), by = c("wflow_id", ".config")) %>% 
  ggplot(aes(color = wflow_id, x = wflow_id)) +
  geom_pointrange(aes(y=mean, ymax=mean+std_err, ymin=mean-std_err)) +
  facet_wrap(~ .metric, scales = "free_y") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  xlab(label = NULL) + ylab(label = NULL) 
```

- The model bp_3D1 has better performance. 

**Fit both best models and examine the number of non-zero coefficients.**  

```{r }
enet_wset_cl %>%
  extract_workflow("all_cont_pairwise_enet") %>%
  finalize_workflow(parameters = enet_best_roc_params %>% slice(1)) %>% 
  fit(dfiiiD) %>% 
  tidy() %>% filter(estimate != 0) %>% nrow()

enet_wset_cl %>%
    extract_workflow("inter_categorical_cont_enet") %>%
    finalize_workflow(parameters = enet_best_roc_params %>% slice(2)) %>% 
  fit(dfiiiD) %>% 
  tidy() %>% filter(estimate != 0) %>% nrow()
```

- There are slightly more features in inter_categorical_cont_enet. 

**Finalize the workflow for the best model and retrain with resampling to save predictions.** 

```{r}
enet_best_wflow_roc <- "all_cont_pairwise_enet"
enet_best_param <- enet_best_roc_params %>%
      filter(wflow_id==enet_best_wflow_roc) %>%
      select(all_of(names(enet_grid_cl)))

mod_3D_enet_best_wflow <-
  enet_wset_cl %>%
  extract_workflow(enet_best_wflow_roc) %>%
  finalize_workflow(parameters = enet_best_param)
mod_3D_enet_best_wflow$param <- enet_best_param

mod_3D_enet_best_resample <-
  mod_3D_enet_best_wflow %>%
  fit_resamples(
    resamples = cv_folds,
    metrics = my_metrics_cl,
    control = control_resamples(save_pred = T)
  )

mod_3D_enet_best_resample %>% collect_metrics() %>% select(-.config, -.estimator)
```


```{r}
mod_3D_enet_best_wflow %>% readr::write_rds("mod_3D_enet_best_wflow.rds")
mod_3D_enet_best_resample %>% readr::write_rds("mod_3D_enet_best_resample.rds")
```

----

